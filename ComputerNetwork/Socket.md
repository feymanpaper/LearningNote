## 参考链接
https://zhuanlan.zhihu.com/p/610719891
http://www.52im.net/thread-1732-1-1.html
https://juejin.cn/post/7022528754157092900
https://segmentfault.com/a/1190000040600569
https://chennq.com/learn-python/20190516-python-socket.html
https://www.birdpython.com/posts/1/84/
https://chennq.com/learn-python/20190516-python-socket.html
https://segmentfault.com/a/1190000039691657
https://z.itpub.net/article/detail/E07055E62740F8C59E1649519A7F1A41
https://cloud.tencent.com/developer/article/1804413

## Socket基础
### Socket是什么
协议栈其实是位于操作系统中的一些协议的堆叠，这些协议包括 TCP、UDP、ARP、ICMP、IP等。  
**通常某个协议的设计都是为了解决特定问题的，比如：**  
-   1）TCP 的设计就负责安全可靠的传输数据；
-   2）UDP 设计就是报文小，传输效率高；
-   3）_ARP 的设计是能够通过 IP 地址查询物理（Mac）地址；
-   4）ICMP 的设计目的是返回错误报文给主机；
-   5）IP 设计的目的是为了实现大规模主机的互联互通。

应用程序的下面：就是操作系统内部，操作系统内部包括协议栈，协议栈是一系列协议的堆叠。  
操作系统下面：就是网卡驱动程序，网卡驱动程序负责控制网卡硬件，驱动程序驱动网卡硬件完成收发工作。  
在操作系统内部有一块用于存放控制信息的存储空间，这块存储空间记录了用于控制通信的控制信息。其实这些控制信息就是 Socket 的实体，或者说存放控制信息的内存空间就是Socket的实体。

netstat打印：
-   _**1）**_每一行都相当于一个Socket；五元组：协议，本地地址，外部地址，状态，PID
-   _**2）**_每一列也被称为一个元组。

**PS：**有的时候也被叫做四元组，四元组不包括协议
![[window_netstat.png]]

传输层向高层用户**屏蔽**了下面网络核心的细节（如网络拓扑、所采用的路由选择协议等），它使应用进程看见的就是好像在两个运输层实体之间有一条**端到端的逻辑通信信道。**

当传输层采用面向连接的 TCP 协议时，尽管下面的网络是不可靠的（只提供尽最大努力服务），但这种逻辑通信信道就相当于一条全双工的**可靠信道**。

当传输层采用无连接的 UDP 协议时，这种逻辑通信信道是一条**不可靠信道**。


### Socket是如何创建的
Socket 是和应用程序一起创建的。  
应用程序中有一个 socket 组件，在应用程序启动时，会调用 socket 申请创建Socket，协议栈会根据应用程序的申请创建Socket：首先分配一个Socket所需的内存空间，这一步相当于是为控制信息准备一个容器，但只有容器并没有实际作用，所以你还需要向容器中放入控制信息；如果你不申请创建Socket所需要的内存空间，你创建的控制信息也没有地方存放，所以分配内存空间，放入控制信息缺一不可。至此Socket的创建就已经完成了。  
Socket创建完成后，会返回一个Socket描述符给应用程序，这个描述符相当于是区分不同Socket的号码牌。根据这个描述符，应用程序在委托协议栈收发数据时就需要提供这个描述符。

### Socket是如何连接的
实际上这个“连接”是应用程序通过 TCP/IP 协议标准从一个主机通过网络介质传输到另一个主机的过程。

首先，客户端应用程序需要调用 Socket 库中的 connect 方法，提供 socket 描述符和服务器 IP 地址、端口号
```
connect(<描述符>、<服务器IP地址和端口号>)
```
三次握手 
当所有建立连接的报文都能够正常收发之后，此时套接字就已经进入可收发状态了，此时可以认为用一根管理把两个套接字连接了起来。当然，实际上并不存在这个管子。建立连接之后，协议栈的连接操作就结束了，也就是说 connect 已经执行完毕，控制流程被交回给应用程序。

### Socket是如何收发数据的
当控制流程上节中的连接过程回到应用程序之后，接下来就会直接进入数据收发阶段。  
  
数据收发操作是从应用程序调用 write 将要发送的数据交给协议栈开始的，协议栈收到数据之后执行发送操作。  
  
协议栈不会关心应用程序传输过来的是什么数据，因为这些数据最终都会转换为二进制序列，协议栈在收到数据之后并不会马上把数据发送出去，而是会将数据放在发送缓冲区，再等待应用程序发送下一条数据。

### Socket是如何断开连接的
无论哪一方发起断开连接的请求，都会调用 Socket 库的 close 程序。

## 读写Socket时，究竟在读写什么？

### 读写过程
当客户端和服务器使用TCP协议进行通信时，客户端封装一个请求对象req，将请求对象req序列化成字节数组，然后通过套接字socket将字节数组发送到服务器，服务器通过套接字socket读取到字节数组，再反序列化成请求对象req，进行处理，处理完毕后，生成一个响应对应res，将响应对象res序列化成字节数组，然后通过套接字将字节数组发送给客户端，客户端通过套接字socket读取到字节数组，再反序列化成响应对象。

我们平时用到的套接字其实只是一个引用(一个对象ID)，这个套接字对象实际上是放在操作系统内核中。这个套接字对象内部有两个重要的缓冲结构，一个是读缓冲(read buffer)，一个是写缓冲(write buffer)，它们都是有限大小的数组结构。  
  
当我们对客户端的socket写入字节数组时(序列化后的请求消息对象req)，是将字节数组拷贝到内核区套接字对象的write buffer中，内核网络模块会有单独的线程负责不停地将write buffer的数据拷贝到网卡硬件，网卡硬件再将数据送到网线，经过一些列路由器交换机，最终送达服务器的网卡硬件中。  

同样，服务器内核的网络模块也会有单独的线程不停地将收到的数据拷贝到套接字的read buffer中等待用户层来读取。最终服务器的用户进程通过socket引用的read方法将read buffer中的数据拷贝到用户程序内存中进行反序列化成
请求对象进行处理。然后服务器将处理后的响应对象走一个相反的流程发送给客户端，这里就不再具体描述。
![[socket读写过程.png]]

### 阻塞
我们注意到write buffer空间都是有限的，所以如果应用程序往套接字里写的太快，这个空间是会满的。一旦满了，写操作就会阻塞，直到这个空间有足够的位置腾出来。不过有了NIO(非阻塞IO)，写操作也可以不阻塞，能写多少是多少，通过返回值来确定到底写进去多少，那些没有写进去的内容用户程序会缓存起来，后续会继续重试写入。  
  
同样我们也注意到read buffer的内容可能会是空的。这样套接字的读操作(一般是读一个定长的字节数组)也会阻塞，直到read buffer中有了足够的内容(填充满字节数组)才会返回。有了NIO，就可以有多少读多少，无须阻塞了。读不够的，后续会继续尝试读取。

当写缓冲的内容拷贝到网卡后，是不会立即从写缓冲中将这些拷贝的内容移除的，而要等待对方的ack过来之后才会移除。如果网络状况不好，ack迟迟不过来，写缓冲很快就会满的。

### 分块
图中的消息req被拷贝到网卡的时候变成了大写的REQ，这是为什么呢？因为这两个东西已经不是完全一样的了。内核的网络模块会将缓冲区的消息进行分块传输，如果缓冲区的内容太大，是会被拆分成多个独立的小消息包的。并且还要在每个消息包上附加上一些额外的头信息，比如源网卡地址和目标网卡地址、消息的序号等信息，到了接收端需要对这些消息包进行重新排序组装去头后才会扔进读缓冲中。

### 速率
还有个问题那就是如果读缓冲满了怎么办，网卡收到了对方的消息要怎么处理？一般的做法就是丢弃掉不给对方ack，对方如果发现ack迟迟没有来，就会重发消息。那缓冲为什么会满？是因为消息接收方处理的慢而发送方生产的消息太快了，这时候tcp协议就会有个动态窗口调整算法来限制发送方的发送速率，使得收发效率趋于匹配。如果是udp协议的话，消息一丢那就彻底丢了

## 深入操作系统，一文搞懂Socket到底是什么

### socket编程

创建个关于TCP的socket
```
sock_fd = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP);
```

上面这个方法会返回socket_fd，它是socket文件的句柄，是个数字，相当于socket的身份证号。得到了socket_fd之后，对于服务端，就可以依次执行bind(), listen(), accept()方法，然后坐等客户端的连接请求。

**对于客户端，得到socket_fd之后，你就可以执行connect()方法向服务端发起建立连接的请求，此时就会发生TCP三次握手**（如下图所示）
[[网络是怎么样连接的]]讲解各个socket方法
![[socket_tcp三次握手.png]]
连接建立完成后，客户端可以执行send() 方法发送消息，服务端可以执行recv()方法接收消息，反过来，服务器也可以执行send()，客户端执行recv()方法。

### socket该怎么设计 

```
Read, Write, Send, Recv
```
**这里有两个问题：**
-   **_1）_**接收端和发送端可能不止一个，因此我们需要一些信息做下区分，这个大家肯定很熟悉，可以用IP和端口。IP用来定位是哪台电脑，端口用来定位是这台电脑上的哪个进程；
-   **_2）_**发送端和接收端的传输方式有很多区别，可以是可靠的TCP协议，也可以是不可靠的UDP协议，甚至还需要支持基于icmp协议的ping命令。

sock:写过代码的都知道，为了支持这些功能，我们需要定义一个数据结构去支持这些功能。这个数据结构，叫sock。
**第一个问题，我们可以在sock里加入IP和端口字段：**

**第二个问题：**我们会发现这些协议虽然各不相同，但还是有一些功能相似的地方，比如收发数据时的一些逻辑完全可以复用。按面向对象编程的思想，我们可以将不同的协议当成是不同的对象类（或结构体），将公共的部分提取出来，通过"继承"的方式，复用功能。
```
继承sock的各类sock
```
sock是最基础的结构，维护一些任何协议都有可能会用到的收发数据缓冲区。

inet_sock特指用了网络传输功能的sock，在sock的基础上还加入了TTL，端口，IP地址这些跟网络传输相关的字段信息。说到这里大家就懵了，难道还有不是用网络传输的？有，比如Unix domain socket，用于本机进程之间的通信，直接读写文件，不需要经过网络协议栈。这是个非常有用的东西，我以后一定讲讲（画饼）。

inet_connection_sock 是指面向连接的sock，在inet_sock的基础上加入面向连接的协议里相关字段，比如accept队列，数据包分片大小，握手失败重试次数等。虽然我们现在提到面向连接的协议就是指TCP，但设计上linux需要支持扩展其他面向连接的新协议，

tcp_sock 就是正儿八经的tcp协议专用的sock结构了，在inet_connection_sock基础上还加入了tcp特有的滑动窗口、拥塞避免等功能。同样udp协议也会有一个专用的数据结构，叫udp_sock。

好了，现在有了这套数据结构，我们将它们跟硬件网卡对接一下，就实现了网络传输的功能。
![[基于各种sock实现网络传输功能.png]]

```
提供socket层
```
可以想象得到，这里面的代码肯定非常复杂，同时还操作了网卡硬件，需要比较高的操作系统权限，再考虑到性能和安全，于是决定将它放在操作系统内核里。

既然网络传输功能做在内核里，那用户空间的应用程序想要用这部分功能的话，该怎么办呢？

这个好办，本着不重复造轮子的原则，我们将这部分功能抽象成一个个简单的接口。以后别人只需要调用这些接口，就可以驱动我们写好的这一大堆复杂的数据结构去发送数据。

那么问题来了，怎么样将这部分功能暴露出去呢？让其他程序员更方便的使用呢？

既然跟远端服务端进程收发数据可以抽象为“读和写”，操作文件也可以抽象为"读和写"，正好有句话叫，"linux里一切皆是文件"，那我们索性，将内核的sock封装成文件就好了。创建sock的同时也创建一个文件，文件有个句柄fd，说白了就是个文件系统里的身份证号码，通过它可以唯一确定是哪个sock。

这个文件句柄fd其实就是 sock_fd = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) 里的sock_fd。

将句柄暴露给用户，之后用户就可以像操作文件句柄那样去操作这个sock句柄。在用户空间里操作这个句柄，文件系统就会将操作指向内核sock结构。

**是的，操作这个特殊的文件就相当于操作内核里对应的sock**

```
 通过文件找到sock
```
有了sock_fd句柄之后，我们就需要提供一些接口方法，让用户更方便的实现特定的网络编程功能。这些接口，我们列了一下，发现需要有send()，recv()，bind(), listen()，connect()这些。

**到这里，我们的内核网络传输功能就算设计完成了。**

现在是不是眼熟了，**上面这些接口方法其实就是socket提供出来的接口。**

**所以说：**socket其实就是个代码库 or 接口层，它介于内核和应用程序之间，提供了一些高度封装过的接口，让我们去使用内核网络传输功能。
[[图解网络]] socket部分
![[socketandsock.png]]
```
基于sock实现网络传输功能
```
到这里，我们应该明白了。我们平时写的应用程序里代码里虽然用了socket实现了收发数据包的功能，但其实真正执行网络通信功能的，不是应用程序，而是linux内核。相当于应用程序通过socket提供的接口，将网络传输的这部分工作外包给了linux内核。

这听起来像不像我们最熟悉的前后端分离的服务架构，虽然这么说不太严谨，但看上去linux就像是被分成了应用程序和内核两个服务。内核就像是后端，暴露了好多个api接口，其中一类就是socket的send()和recv()这些方法。**应用程序就像是前端，负责调用内核提供的接口来实现想要的功能。**

### socket如何实现网络通信
两阶段，分别是**建立连接**和**数据传输**
```
建立连接
```
对于TCP，要传数据，就得先在客户端和服务端中间**建立连接**。
在客户端，代码执行socket提供的connect(sockfd, "ip:port")方法时，会通过sockfd句柄找到对应的文件，再根据文件里的信息指向内核的sock结构。
**通过这个sock结构主动发起三次握手**

在服务端握手次数还没达到"三次"的连接，叫半连接，完成好三次握手的连接，叫全连接。它们分别会用半连接队列和全连接队列来存放，这两个队列会在你执行listen()方法的时候创建好。

**当服务端执行accept()方法时，就会从全连接队列里拿出一条全连接：**
![[半连接和全连接.png]]
```
数据传输
```
为了实现发送和接收数据的功能，sock结构体里带了一个发送缓冲区和一个接收缓冲区，说是缓冲区，但其实就是个链表，上面挂着一个个准备要发送或接收的数据。

当应用执行send()方法发送数据时，同样也会通过sock_fd句柄找到对应的文件，根据文件指向的sock结构，找到这个sock结构里带的发送缓冲区，将数据会放到发送缓冲区，然后结束流程，内核看心情决定什么时候将这份数据发送出去。

接收数据流程也类似，当数据送到linux内核后，数据不是立马给到应用程序的，而是先放在接收缓冲区中，数据静静躺着，卑微的等待应用程序什么时候执行recv()方法来拿一下。

那么问题来了，发送数据是应用程序主动发起，这个大家都没问题。那接收数据呢？数据从远端发过来了，怎么通知并给到应用程序呢？

**这就需要用到等待队列：**
当你的应用进程执行recv()方法尝试获取（阻塞场景下）接收缓冲区的数据时：
-   **_1)_** 如果有数据，那正好，取走就好了。这点没啥疑问；
-   **_2)_** 但如果没数据，就会将自己的进程信息注册到这个sock用的等待队列里，然后进程休眠。如果这时候有数据从远端发过来了，数据进入到接收缓冲区时，内核就会取出sock的等待队列里的进程，唤醒进程来取据。
![[sock_进程等待队列.png]]

recv时无数据进程进入等待队列

有时候，你会看到多个进程通过fork的方式，listen了同一个socket_fd。在内核，它们都是同一个sock，多个进程执行listen()之后，都嗷嗷等待连接进来，所以都会将自身的进程信息注册到这个socket_fd对应的内核sock的等待队列中。

如果这时真来了一个连接，是该唤醒等待队列里的哪个进程来接收连接呢？
**这个问题的答案比较有趣：**

-   **_1)_** 在linux 2.6以前，会唤醒等待队列里的所有进程。但最后其实只有一个进程会处理这个连接请求，其他进程又重新进入休眠，这些被唤醒了又无事可做最后只能重新回去休眠的进程会消耗一定的资源。就好像你在广东的街头，想问路，叫一声靓仔，几十个人同时回头，但你其实只需要其中一个靓仔告诉你路该怎么走。你这种一不小心惊动这群靓仔的场景，在计算机领域中，就叫惊群效应；
-   **_2)_** 在linux 2.6之后，只会唤醒等待队列里的其中一个进程。是的，socket监听的惊群效应问题被修复了。

**服务端 listen 的时候，那么多数据到一个 socket 怎么区分多个客户端的？**
以TCP为例，服务端执行listen方法后，会等待客户端发送数据来。客户端发来的数据包上会有源IP地址和端口，以及目的IP地址和端口，这四个元素构成一个四元组，可以用于唯一标记一个客户端。

服务端会创建一个新的内核sock，并用四元组生成一个hash key，将它放入到一个hash表中。

下次再有消息进来的时候，通过消息自带的四元组生成hash key再到这个hash表里重新取出对应的sock就好了。**所以说服务端是通过四元组来区分多个客户端的。**
![[多个四元组hashkey区分多个客户端.png]]


既然网络传输功能做在内核里，那用户空间的应用程序想要用这部分功能的话，该怎么办呢？

这个好办，本着不重复造轮子的原则，我们将这部分功能抽象成一个个简单的接口。以后别人只需要调用这些接口，就可以驱动我们写好的这一大堆复杂的数据结构去发送数据。

那么问题来了，怎么样将这部分功能暴露出去呢？让其他程序员更方便的使用呢？

既然跟远端服务端进程收发数据可以抽象为“读和写”，操作文件也可以抽象为"读和写"，正好有句话叫，"linux里一切皆是文件"，那我们索性，将内核的sock封装成文件就好了。创建sock的同时也创建一个文件，文件有个句柄fd，说白了就是个文件系统里的身份证号码，通过它可以唯一确定是哪个sock。

这个文件句柄fd其实就是 sock_fd = socket(AF_INET, SOCK_STREAM, IPPROTO_TCP) 里的sock_fd。

将句柄暴露给用户，之后用户就可以像操作文件句柄那样去操作这个sock句柄。在用户空间里操作这个句柄，文件系统就会将操作指向内核sock结构。

**是的，操作这个特殊的文件就相当于操作内核里对应的sock：**

## Socket缓冲区 
每个socket被创建后，无论使用的是TCP协议还是UDP协议，都会创建自己的接收缓冲区和发送缓冲区。当我们调用write()/send() 向网络发送数据时，系统并不会 马上向网络传输数据，而是首先将数据拷贝到发送缓冲区，由系统负责择时发送数据。根据我们选用的网络协议以及阻塞模式，系统会有不同的处理。

**socket 缓冲区**（是队列）。
用户**发送**消息的时候写给 send buffer（发送缓冲区）。
用户**接收**消息的时候，是从 recv buffer（接收缓冲区）中读取数据。
也就是说**一个socket ，会带有两个缓冲区**，一个用于发送，一个用于接收。因为这是个先进先出的结构，有时候也叫它们**发送、接收队列**。

这些socket缓冲区特性可整理如下：

1.  socket缓冲区在每个套接字中单独存在；
2.  socket缓冲区在创建套接字时自动生成；
3.  即使关闭套接字也会继续传送发送缓冲区中遗留的数据；
4.  关闭套接字将丢失接收缓冲区中的数据。

![[socket_buffer.png]]

### TCP阻塞和非阻塞模式下的数据发送

1.  阻塞模式下，调用write()/send()后程序将阻塞，如果发送缓冲区的可用长度大于待发送的数据，则数据将全部被拷贝到发送缓冲区，等待系统将发送缓冲区内的数据发送出去，当数据全部被拷贝到发送缓冲区后阻塞状态将消失；如果发送缓冲区的长度小于待发送的数据长度，则数据能拷贝多少就先拷贝多少（分批拷贝），一直等待直到数据可以全部被拷贝到发送缓冲区为止才可调用返回。write()/send()调用返回后并不能保证数据已经发送到对方缓冲区了，只能保证数据成功拷贝到发送缓冲区了，至于传输可靠性方面那就是由系统的协议栈来保证。
2.  非阻塞模式下，调用write()/send()后，如果发送缓冲区剩余大小大于待发送的数据大小，那数据将完整拷贝到发送缓冲区，如果发送缓冲区剩余大小小于待发送的数据大小，那本次write()/send()则为尽可能拷贝，有多少空间就拷贝多少数据，返回值为均为成功拷贝到发送缓冲区的数据长度。
3.  当接收端不接收数据，或者处理速率比发送方的发送速率低导致其接收缓冲区已满（接收窗口win=0），进而导致数据发送方的发送缓冲区的数据不断堆积进而缓冲区满，此时我们再调用write()/send()都将阻塞等待。
4.  系统将发送缓冲区的数据通过网卡发到网络了，系统也不会立即将刚发送的数据从缓冲区中移除，只有当接收方回复了ack，我们才能认为对方收到了我们发送的信息，否则刚发送的数据必须还保留在发送缓冲区等待重传。当系统收到接收方对刚发送数据的ack后，才会移除发送缓冲区内对应的数据，腾出空间。
5.  当启用了Nagle算法后，数据会倾向于堆积到一定大小或超时后才真正往网络发送数据，因此启用Nagle算法后的发送缓冲区更容易发生数据堆积。
6.  因为发送缓冲区满导致write()/send()一直无法返回，这个可以通过setsockopt的参数 SO_SNDTIMEO来做超时处理，如果有数据成功拷贝到发送缓冲区，那超时后的返回值是成功拷贝到发送缓冲区的数据长度，如果没有数据拷贝成功，此时的超时后返回值为-1，errno为EAGAIN 或 EWOULDBLOCK，表现就是跟非阻塞模式的write()/send()是一样的。如果不设置默认就是永不超时。
7.  socket关闭时，但发送缓冲区中的数据仍未完全成功发送出去，那么这些数据将由系统负责把数据可靠地发送给对方。

### TCP阻塞和非阻塞模式下的数据接收

1.  调用read()/recv()时，如果模式选择的是阻塞模式，那么当接收缓冲区没数据时，程序就会一直拥塞等待，直到有数据可读为止，每次读的数据大小由进程控制，一般都需要分批读取，read()/recv()成功返回时的返回值是成功读取到的数据的长度；如果模式选择的是非阻塞模式，那么程序调用read()/recv()调用返回的返回值是成功读取的字节数，如果没数据可读时同样是马上返回，此时的返回值为0。
2.  当程序并没有及时读取接收缓冲区中的数据，那缓冲区可利用的空间逐渐变小，直到缓冲区满，但TCP套接口接收缓冲区不可能溢出。这是因为TCP有流量控制策略，根据TCP的流量控制中滑动窗口机制，接收方会捎到窗口大小给发送方，如果缓冲区空间为0发送方也能及时知道停止发送。
3.  当socket关闭时，如果接收缓冲区还有数据没读取，那么这部分数据将被丢弃。
4.  跟TCP阻塞写相同，我们可以通过setsockopt的参数 SO_RCVTIMEO来对阻塞模式的读做超时处理，如果一段时间内没有数据读取成功，此时的超时后返回值为-1，errno为EAGAIN 或 EWOULDBLOCK。
5.  recv的第四个参数若为MSG_WAITALL，则在阻塞模式下不等到指定数目的数据不会返回，除非超时时间到。当然如果对方关闭了，即使超时时间未到，recv 也返回0。



### UDP阻塞和非阻塞下的数据发送接收

1.  UDP套接口有发送缓冲区大小（SO_SNDBUF修改），不过它仅仅是写到套接口的UDP数据报的大小上限，即UDP没有发送缓冲区。如果一个应用程序写一个大于套接口发送缓冲区大小的数据报，内核将返回EMSGSIZE错误。
2.  从UDP套接口write成功返回仅仅表示用户写入的数据报或者所有片段已经加入到数据链路层的输出队列。如果该队列没有足够的空间存放该数据包或者它的某个片段，内核通常返回给应用进程一个ENOBUFS错误。
3.  UDP是没有流量控制的：较快的发送端可以很容易淹没较慢的接收端，当接收缓冲区满后，后面收到的数据报都将丢弃。
4.  UDP存在丢包的可能：调用recvfrom方法接收到数据后，处理数据花费时间太长，再次调用recvfrom，两次调用间隔里，发过来的包可能丢失。处理方法是调大接收缓冲区或者通过将接收到数据存入一个缓冲区，并迅速返回继续recvfrom。

### 阻塞和非阻塞本质
![[block_and_noblock.png]]
阻塞：阻塞的本质是，进程因为资源等待而主动让出CPU，进程从运行队列删除，幷加入到等待队列，然后等待资源。等超时或数据资源到来则唤醒进程继续执行，若有数据可读那就把数据拷贝给进程，无数据可读但超时了则返回进程继续执行后面的逻辑。

非阻塞：本质是应用进程掌控读取数据的节奏，通过轮训的方式查询数据是否可读，进程始终占用着CPU，能比较好地满足高性能进程需求，执行效率高（数据没到位，进程可以继续处理其他业务，无需阻塞其他业务进行）。

  
-   TCP协议本身是为了保证可靠传输,并不等于应用程序用tcp发送数据就一定是可靠的，必须要容错；
-   send（）只负责拷贝，拷贝到内核就返回
-   此次send（）调用所触发的程序错误，可能会在本次返回，也可能在下次调用网络IO函数的时候被返回。
-   在进行TCP协议传输的时候，要注意数据流传输的特点，recv和send不一定是一一对应的（一般情况下是一一对应），也就是说并不是send一次，就一定recv一次就接收完，有可能send一次，recv多次才接收完，也可能send多次，一次recv就接收完了。TCP协议会保证数据的有序完整的传输，但是如何去正确完整的处理每一条信息，是开发人员的事情。

> 服务器在循环recv，recv的缓冲区大小为100byte，客户端在循环send，每次send 6byte数据，则recv每次收到的数据可能为6byte，12byte，18byte，这是随机的，编程的时候注意正确的处理

### 其余问题
**执行 send 发送的字节，会立马发送吗？**

答案是不确定！执行 send 之后，数据只是拷贝到了socket 缓冲区。至于什么时候会发数据，发多少数据，**全听操作系统安排**。

在用户进程中，程序通过操作 socket 会从用户态进入内核态，而 send方法会将数据一路传到传输层。在识别到是 TCP协议后，会调用 tcp_sendmsg 方法。
在 tcp_sendmsg 中， 核心工作就是将待发送的数据组织按照先后顺序放入到发送缓冲区中， 然后根据实际情况（比如拥塞窗口等）判断是否要发数据。如果不发送数据，那么此时直接返回。
```c
 int tcp_sendmsg()
 {  
   if (skb_availroom(skb) > 0) {
     // ..如果有足够缓冲区就执行balabla
   } else {
     // 如果发送缓冲区没空间了，那就等到有空间，至于等的方式，分阻塞和非阻塞
     if ((err = sk_stream_wait_memory(sk, &timeo)) != 0)
         goto do_error;
   }   
 }  
```
`sk_stream_wait_memory` 会根据`socket`是否阻塞，来决定是**一直等**，还是**等一会就返回**。
```c
int sk_stream_wait_memory(struct sock *sk, long *timeo_p)
 {
   while (1) {
     // 非阻塞模式时，会等到超时返回 EAGAIN
     if (等待超时))
       return -EAGAIN;     
      // 阻塞等待时，会等到发送缓冲区有足够的空间了，才跳出
     if (sk_stream_memory_free(sk) && !vm_wait)
       break;
   }
   return err;
 }
```

**如果发送缓冲区空间不足，或者满了，执行发送，会怎么样？**
如果此时 socket 是阻塞的，那么程序会在那**干等、死等**，直到释放出新的缓存空间，就继续把数据拷进去，然后**返回**。
如果此时 socket 是非阻塞的，程序就会**立刻返回**一个 `EAGAIN` 错误信息，意思是 `Try again` , 现在缓冲区满了，你也别等了，待会再试一次。
![[socket_send.png]]

**如果接收缓冲区为空，执行 recv 会怎么样？**
如果此时 socket 是阻塞的，那么程序会在那**干等**，直到接收缓冲区有数据，就会把数据从接收缓冲区拷贝到用户缓冲区，然后**返回**。
如果此时 socket 是非阻塞的，程序就会**立刻返回**一个 `EAGAIN` 错误信息。

**如果接收缓冲区还有数据，执行close了，会怎么样？**
首先我们要知道，**一般正常情况下，发送缓冲区和接收缓冲区 都应该是空的。**
正常情况下，如果 `socket` 缓冲区**为空**，执行 `close`。就会触发四次挥手。

如果发送、接收缓冲区长时间非空，说明有数据堆积，这往往是由于一些网络问题或用户应用层问题，导致数据没有正常处理。
`socket close` 时，主要的逻辑在 `tcp_close()` 里实现。
如果接收缓冲区**还有数据未读**，会先把接收缓冲区的数据清空，然后给对端发一个RST。
如果接收缓冲区是**空的**，那么就调用 `tcp_send_fin()` 开始进行四次挥手过程的第一次挥手。
```c
void tcp_close(struct sock *sk, long timeout)
 {
   // 如果接收缓冲区有数据，那么清空数据
   while ((skb = __skb_dequeue(&sk->sk_receive_queue)) != NULL) {
     u32 len = TCP_SKB_CB(skb)->end_seq - TCP_SKB_CB(skb)->seq -
         tcp_hdr(skb)->fin;
     data_was_unread += len;
     __kfree_skb(skb);
   }
 ​
    if (data_was_unread) {
     // 如果接收缓冲区的数据被清空了，发 RST
     tcp_send_active_reset(sk, sk->sk_allocation);
    } else if (tcp_close_state(sk)) {
     // 正常四次挥手， 发 FIN
     tcp_send_fin(sk);
   }
   // 等待关闭
   sk_stream_wait_close(sk, timeout);
 }
```
 **如果发送缓冲区有数据时，执行close了，会怎么样？**
 以前以为，这种情况下，内核会把发送缓冲区数据清空，然后四次挥手。

但是发现源码**并不是这样的**。

```c
 void tcp_send_fin(struct sock *sk)
 {
   // 获得发送缓冲区的最后一块数据
   struct sk_buff *skb, *tskb = tcp_write_queue_tail(sk);
   struct tcp_sock *tp = tcp_sk(sk);
 ​
   // 如果发送缓冲区还有数据
   if (tskb && (tcp_send_head(sk) || sk_under_memory_pressure(sk))) {
     TCP_SKB_CB(tskb)->tcp_flags |= TCPHDR_FIN; // 把最后一块数据值为 FIN 
     TCP_SKB_CB(tskb)->end_seq++;
     tp->write_seq++;
   }  else {
     // 发送缓冲区没有数据，就造一个FIN包
   }
   // 发送数据
   __tcp_push_pending_frames(sk, tcp_current_mss(sk), TCP_NAGLE_OFF);
 }
```
此时，还有些数据没发出去，内核会把发送缓冲区最后一个数据块拿出来。然后置为 FIN。

`socket` 缓冲区是个**先进先出**的队列，这种情况是指，内核会等待TCP层安静地把发送缓冲区数据都**发完**，最后再执行 四次挥手的第一次挥手（FIN包）。

有一点需要注意的是，只有在**接收缓冲区为空的前提下**，我们才有可能走到 `tcp_send_fin()` 。而只有在进入了这个方法之后，我们才有可能考虑**发送缓冲区是否为空**的场景。

**UDP也有缓冲区吗**
UDP socket 也是 socket，一个socket 就是会有收和发两个缓冲区。**跟用什么协议关系不大**。
事实上，UDP不仅`有`发送缓冲区，也`用`发送缓冲区。
一般正常情况下，会把数据直接拷到发送缓冲区后直接发送。
还有一种情况，是在发送数据的时候，设置一个 `MSG_MORE` 的标记
```c
ssize_t send(int sock, const void *buf, size_t len, int flags); // flag 置为 MSG_MORE
```

大概的意思是告诉内核，待会还有其他**更多消息**要一起发，先别着急发出去。此时内核就会把这份数据先用**发送缓冲区**缓存起来，待会应用层说ok了，再一起发。

我们可以看下源码。

```c
int udp_sendmsg()
{
	// corkreq 为 true 表示是 MSG_MORE 的方式，仅仅组织报文，不发送；
	int corkreq = up->corkflag || msg->msg_flags&MSG_MORE；
    
	//  将要发送的数据，按照MTU大小分割，每个片段一个skb；并且这些
	//  skb会放入到套接字的发送缓冲区中；该函数只是组织数据包，并不执行发送动作。
	err = ip_append_data(sk, fl4, getfrag, msg->msg_iov, ulen,
			     sizeof(struct udphdr), &ipc, &rt,
			     corkreq ? msg->msg_flags|MSG_MORE : msg->msg_flags);

	// 没有启用 MSG_MORE 特性，那么直接将发送队列中的数据发送给IP。 
    if (!corkreq)
		err = udp_push_pending_frames(sk);
}
```
因此，不管是不是 `MSG_MORE`， UDP都会先把数据放到发送队列中，然后根据实际情况再考虑是不是立刻发送。

而我们大部分情况下，都不会用 `MSG_MORE`，也就是来一个数据包就直接发一个数据包。从这个行为上来说，**虽然UDP用上了发送缓冲区，但实际上并没有起到"缓冲"的作用。**

操作系统在收到 UDP 报文后，会将其插入到队列里。接收端的skbuff(套接字缓冲区）采用了链式结构来记录每一个到达的UDP包，在每个UDP包中就有了消息头（消息来源地址，端口等信息。**队列里的每一个元素就是一个 UDP 报文**，这样当用户调用 recvfrom() 系统调用读数据的时候，就会从队列里取出一个数据，然后从内核里拷贝给用户缓冲区。
![[udp队列.png]]
因此，udp是不会粘包的

**关掉 Nagle 算法就不会粘包了吗？**
在 Nagle 算法开启的状态下，数据包在以下两个情况会被发送：

-   如果包长度达到`MSS`（或含有`Fin`包），立刻发送，否则**等待**下一个包到来；如果下一包到来后两个包的总长度超过`MSS`的话，就会进行拆分发送；
-   等待超时（一般为`200ms`），第一个包没到`MSS`长度，但是又迟迟等不到第二个包的到来，则立即发送。

就算关闭 Nagle 算法，接收数据端的应用层没有及时读取 TCP Recv Buffer 中的数据，还是会发生粘包。

**UDP会粘包吗？**
跟 `TCP` 同为传输层的另一个协议，**UDP，User Datagram Protocol**。用户数据包协议，是面向无连接，不可靠的，基于**数据报**的传输层通信协议。

基于**数据报**是指无论应用层交给 UDP 多长的报文，UDP 都照样发送，即一次发送一个报文。至于如果数据包太长，需要分片，那也是IP层的事情，大不了效率低一些。UDP 对应用层交下来的报文，既不合并，也不拆分，而是保留这些报文的边界。而接收方在接收数据报的时候，也不会像面对 TCP 无穷无尽的二进制流那样不清楚啥时候能结束。正因为**基于数据报**和**基于字节流**的差异，**TCP 发送端发 10 次字节流数据，而这时候接收端可以分 100 次去取数据，每次取数据的长度可以根据处理能力作调整；但 UDP 发送端发了 10 次数据报，那接收端就要在 10 次收完，且发了多少，就取多少，确保每次都是一个完整的数据报。**
![[ip报头.png]]

![[udp报头.png]]
在报头中有`16bit`用于指示 **UDP 数据报文的长度**，假设这个长度是 n ，以此作为**数据边界**。因此在接收端的应用层能清晰地将不同的数据报文区分开，从报头开始取 n 位，就是一个**完整的**数据报，从而避免粘包和拆包的问题。

当然，就算没有这个位（**16位 UDP 长度**），因为 IP 的头部已经包含了数据的**总长度**信息，此时如果 IP 包（网络层）里放的数据使用的协议是 UDP（传输层），那么这个**总长度**其实就包含了 UDP 的头部和 UDP 的数据。

因为 UDP 的头部长度固定为 8 字节（ 1 字节= 8 位，8 字节= 64 位，上图中除了`数据和选项`以外的部分），那么这样就很容易的算出 UDP 的数据的长度了。因此说 UDP 的长度信息其实是冗余的。

UDP Data 的长度 = IP 总长度 - IP Header 长度 - UDP Header 长度

TCP首部里是没有长度这个信息的，跟UDP类似，同样可以通过下面的公式获得当前包的TCP数据长度。

TCP Data 的长度 = IP 总长度 - IP Header 长度 - TCP Header 长度。
![[TCP报头.png]]
跟 UDP 不同在于，TCP 发送端在发的时候就**不保证发的是一个完整的数据报**，仅仅看成一连串无结构的字节流，这串字节流在接收端收到时哪怕知道长度也没用，因为它很可能只是某个完整消息的一部分。

**为什么长度字段冗余还要加到 UDP 首部中？**
关于这一点，查了很多资料，`《 TCP-IP 详解（卷2）》`里说可能是因为要用于计算校验和。也有的说是因为UDP底层使用的可以不是IP协议，毕竟 IP 头里带了总长度，正好可以用于计算 UDP 数据的长度，万一 UDP 的底层不是IP层协议，而是其他网络层协议，就不能继续这么计算了。

以下有问题，udp是放在链式队列里的，不会粘包
~~(但我觉得，最重要的原因是，IP 层是网络层的，而 UDP 是传输层的，到了传输层，数据包就已经不存在IP头信息了，那么此时的UDP数据会被放在 UDP 的 `Socket Buffer` 中。当应用层来不及取这个 UDP 数据报，那么两个数据报在数据层面其实都是一堆 01 串。此时读取第一个数据报的时候，会先读取到 UDP 头部，**如果这时候 UDP 头不含 UDP 长度信息，那么应用层应该取多少数据才算完整的一个数据报呢**？因此 UDP 头的这个长度其实跟 TCP 为了防止粘包而在消息体里加入的边界信息是起一样的作用的。)~~

**UDP报文的长度字段是冗余的**，因为UDP的长度完全可以通过当时在IP层就把报头取出来就可以计算出数据的长度，TCP也是一样。但是为什么没有这样做？我的想法是，每一层的分工不一样，IP层不会为了TCP提前计算出数据的长度，而且在socket缓冲区时数据已经不包含TCP和UDP头了。因此协议栈并没有实现这样的机制。而是将TCP粘包交给上层应用层（如http）来处理。至于UDP，会保证及时发送，及时取，因此也不会有粘包的问题。

**ip层会有粘包问题吗？**
IP 层会对大包进行切片，是不是也有粘包问题？

先说结论，不会。首先前文提到了，粘包其实是由于使用者无法正确区分消息边界导致的一个问题。
![[ip分片.png]]
先看看 IP 层的切片分包是怎么回事。
-   如果消息过长，`IP层`会按 **MTU 长度**把消息分成 **N 个切片**，每个切片带有自身在**包里的位置（offset）**和**同样的IP头信息**。
-   各个切片在网络中进行传输。每个数据包切片可以在不同的路由中流转，然后**在最后的终点汇合后再组装**。
-   在接收端收到第一个切片包时会申请一块新内存，创建IP包的数据结构，等待其他切片分包数据到位。
-   等消息全部到位后就把整个消息包给到上层（传输层）进行处理。

可以看出整个过程，`IP 层`从按长度切片到把切片组装成一个数据包的过程中，都只管运输，都不需要在意消息的边界和内容，都不在意消息内容了，那就不会有粘包一说了。


## Socket编程
创建socket的时候需要指定socket的类型，一般有三种：
1.  SOCK_STREAM：面向连接的稳定通信，底层是 TCP 协议，参数默认是这个
2.  SOCK_DGRAM：无连接的通信，底层是 UDP 协议，需要上层的协议来保证可靠性。
3.  SOCK_RAW：更加灵活的数据控制，能让你指定 IP 头部
还需要指定套接字的家族，有两种
1.  基于文件类型的套接字家族：AF_UNIX
2.  基于网络类型的套接字家族：AF_INET，最为广泛，本文也将使用这个

![[基于tcp的socket通信.png]]


1、send和sendall的区别
发送方使用sendall，不要使用send。
sendall在返回的时候，会确保数据已传给tcp/ip；send并不一定会把数据全部发完，而是发一次，就返回发送的数据长度。
在一般的程序设计中，我们更多场景是使用sendall，使用send就需要自己处理未发送的数据。`sendall()`是对`send()`的包装，完成了用户需要手动完成的部分，它会自动判断每次发送的内容量，然后从总内容中删除已发送的部分，将剩下的继续传给`send()`进行发送；

2、recv是拷贝数据，不是接收数据，每次拷贝多少？
recv并不是取完对方发送的数据，而是取一次。取多少字节，取决于recv的参数buffsize recv(SOCK_BUFF_SIZE)。如果对方发送了超过buffsize的数据，recv需要多次调用，用户自己组装数据，自己写算法确保数据完整性。即使对方发送的数据没有超过buffsize，也有情况需要多次调用recv，因为发送的数据可能超过了当时的tcpip的承载MTU最大传输量。一般recv都是阻塞型的（可以设置）。

3.a bytes-like object is required, not 'str'
send和sendall传送的数据类型是字节型，不是字符串，发送前需要encode（例如’utf-8‘），recv后需要decode（‘utf-8’）

### 粘包问题
1.  发送端为了将多个发往接收端的包，更有效的发到对方，使用了优化方法（Nagle算法），将多次间隔较小、数据量小的数据包，合并成一个大的数据包发送(把发送端的缓冲区填满一次性发送)。
2.  接收端底层会把tcp段整理排序交给缓冲区，这样接收端应用程序从缓冲区取数据就只能得到整体数据而不知道怎么拆分（tcp协议是流式协议，多条消息之间没有边界）

现象1:recv端产生的黏包现象
![[recv产生的粘包.png]]
现象2：send端可能产生的粘包现象（连续send少量数据发到输出缓冲区，可能在缓冲区不断积压，多次写入的数据一次性发到网络，这取决于当前的网络状态）
![[send产生的粘包.png]]
第一次recv会拿到全部数据，第二次recv会一直阻塞直到管道关闭，会返回空字符串
When no data is available, block untilat least one byte is available or until the remote end is closed.  
当缓冲区没有数据可取时，recv会一直处于阻塞状态，直到缓冲区至少有一个字节数据可取，或者远程端关闭。  
When the remote end is closed and all data is read, return the empty string.  
关闭远程端并读取所有数据后，返回空字符串。

```
pack负责将不同的变量打包在一起，成为一个字节字符串，即类似于C语言中的字节流  
struct.pack(fmt, v1, v2, ...)

unpack将字节字符串解包成为变量  
struct.unpack(fmt, string) 

计算按照格式fmt打包的结果有多少个字节，这个打包格式fmt确定了将变量按照什么方式打包成字节流。其包含了一系列的格式字符串  
struct.calcsize(fmt)
```
 
解决：
-   固定长度的消息；
-   特殊字符作为边界；
-   自定义消息结构：头部固定长度，表示体的数据长度，但是如果数据太大超过限制就有问题；头部变长，加上终结符，能解决上述问题，参考http

能否按序到达？
没有处理小于4字节的

